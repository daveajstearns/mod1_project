{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Jupyter Notebook  \n",
    "## Module 1 Project  \n",
    "#### David Stearns  \n",
    "#### Chris Fiorentine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.7/site-packages (2.22.0)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests) (2.8)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests) (2019.9.11)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests) (1.24.2)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "# Installing of the possible packages needed\\n\",\n",
    "!pip install requests\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import config\n",
    "% matplotlib inline\n",
    "sns.set(style='darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of 500 urls for the 500 pages of data I need\n",
    "# For each number in the range of [1-501) add it where 'i' is in the url below\n",
    "# Using F String Interpolation to append private keys\n",
    "url1 = (\\\"http://api.themoviedb.org/3/discover/movie?api_key={api}{synt}\\\").format(api=config.api, synt='&sort_by=popularity.desc&page=')\n",
    "url_list = []\n",
    "num_list = list(range(1,501))\n",
    "url_list2 = [str(num) for num in num_list]\n",
    "for i in url_list2:\n",
    "    url_list.append(url1 + i)\n",
    "# url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAPI(list, key):\n",
    "    movies = []\n",
    "    # Creating a new list for store all of my url get requests\\n\",\n",
    "    for url in list:\n",
    "        movies.append(requests.get(url))\n",
    "        \n",
    "    # Creating a new list to store all of my response-object-JSON's from the get requests\n",
    "    movies_json = []\n",
    "    for movie in movies:\n",
    "        movies_json.append(movie.json())\n",
    "    # New list to store all of the 'results' for each movie\n",
    "    movies_results = []\n",
    "    for item in movies_json:\n",
    "    # Using concatenation, not appending\n",
    "        movies_results = movies_results + item['key']\n",
    "    return movie_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming the code added 10,000 separate dictionaries to this list\n",
    "# Not 500 separate lists (1 list per page)\n",
    "len(movies_results)\n",
    "# Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe from data\n",
    "movies_df = pd.DataFrame.from_dict(movies_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the list of 10k movies by popularity\n",
    "# Permanently changing the data structure via 'inplace=True'\n",
    "movies_df.sort_values(by='popularity', ascending=False, inplace=True)\n",
    "movies_df.set_index('title', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "movies_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving a dataframe as a csv containing info from all of the movies via Discover API on TMDB\n",
    "# movies_df[['popularity']].to_csv(r'/Users/davestearns/Documents/FlatIron/Mod1 Project/mod1_project-master\\\\popularity_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving IMDB ID's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the big list of links made from web scraping\n",
    "big_links = pd.read_csv('big_links.csv')\n",
    "# Reducing the DF to include just the links as a column\n",
    "# Originally had a column named 'Unnamed: 0' was a second index column - unnecessary\n",
    "big_links = big_links[['0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing column name\n",
    "big_links.rename(columns = {'0':'links'}, inplace=True)\n",
    "imdb_id = []\n",
    "# Taking ID's from index 10-4989\n",
    "for i in big_links['links']:\n",
    "    imdb_id.append(i[37:46])\n",
    "imbd_id = imdb_id[10:5000]\n",
    "# imdb_id\n",
    "# Making the ID's a dataframe\n",
    "imdb_df = pd.DataFrame(imbd_id)\n",
    "# imdb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a CSV into Pandas\n",
    "# File contains bs4 object strings\n",
    "soups = pd.read_csv('soups_file.csv')\n",
    "# Reducing the dataframe\n",
    "soups = soups[['1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding column to the soups DF to include the IMDB ID\n",
    "soups['id'] = imdb_df\n",
    "# Making a dataframe of just the soup object strings\n",
    "clean = soups.iloc[0: , 0:1]\n",
    "# clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling the 'Worldwide Revenue' data from BoxOfficeMojo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function to clean up the process\n",
    "# In this case, str_list = clean\n",
    "# Index will refer to which index we want to use to pull $ info from list of 'money'\n",
    "# Index = int\n",
    "revs_list = []\n",
    "def wwRevs(str_list, index):\n",
    "    bs4 = str_list\n",
    "    win1 = []\n",
    "    for bs in bs4:\n",
    "        win1.append(BeautifulSoup(bs, 'html.parser'))\n",
    "    # Loading in bs4 objects from the dataframe to new list\n",
    "    # Index 2\n",
    "    win2 = []\n",
    "    for won in win1:\n",
    "        try:\n",
    "            win2.append(won.findAll('span', class_= 'money')[index].text.strip('$'))\n",
    "        except IndexError:\n",
    "            type(win2[0])\n",
    "    # Cleaning numbers\n",
    "    # Transforming all str --> int\n",
    "    # Removing commas in numbers\n",
    "    for num in win2:\n",
    "        revs_list.append(int(num.replace(',', '')))\n",
    "    return revs_list[0]\n",
    "# Returns one value from this list as a check to make sure it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descStats(revs_list):\n",
    "    # Turn list into PD df\n",
    "    wwRev_df = pd.DataFrame(revs_list)\n",
    "    return (\n",
    "            wwRev_df.mean(),\n",
    "            wwRev_df.median(),\n",
    "            wwRev_df.mode(),\n",
    "            wwRev_df.describe(),\n",
    "            wwRev_df.sum()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index2 = wwRevs(clean['1'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revs_df = pd.DataFrame(revs_list[0:4990])\n",
    "revs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index2_stats = descStats(revs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the column to something more descriptive\n",
    "revs_df.rename(columns = {0:'ww_rev'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column for the IMDB ID\n",
    "# Will be using this to merge the dataframes later\n",
    "revs_df['id'] = imdb_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of IMDB ID's to add in to the API url to find data\n",
    "id_list = imdb_df[0].to_list()\n",
    "id_list\n",
    "movie_list = []\n",
    "for i in id_list:\n",
    "    movie_list.append(str(f\"https://api.themoviedb.org/3/find/{i}?api_key=41f8edb29d800a88a356853208f114f0&language=en-US&external_source=imdb_id\\\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good candidate for `cleanAPI()`  \n",
    "*The cell below may take some time depending on the size of your data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies8 = []\n",
    "for movie in movie_list:\n",
    "    movies8.append(requests.get(movie))\n",
    "# Creating a new list to store all of my response-object-JSON's from the get requests\n",
    "movies_json8 = []\n",
    "for movie in movies8:\n",
    "    movies_json8.append(movie.json())\n",
    "# New list to store all of the 'results' for each movie\n",
    "movies_results8 = []\n",
    "for item in movies_json8:\n",
    "# Using concatenation, not appending\n",
    "    movies_results8 = movies_results8 + item['movie_results']\n",
    "# Creating PD df from results\n",
    "movies_df8 = pd.DataFrame.from_dict(movies_results8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the current 'id' column and replacing it with a more accurate one\n",
    "# New column has the IMDB ID's\n",
    "movies_df8.drop(['id'], axis = 1, inplace=True)\n",
    "movies_df8['id'] = id_list[0:4982]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will need to apply the ID column to this PD DF to merge the wwRev and the API table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the new PD df with the dataframe containing revenue data\n",
    "# Merging on 'id' column\n",
    "movies_test = movies_df8\n",
    "movies_final = movies_test.merge(revs_df, on='id')\n",
    "movies_final.set_index('title', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_final.sort_values('ww_rev', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies_final.to_csv(r'/Users/davestearns/Documents\\\\movies_final9000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a more simple df\n",
    "movies_finale = movies_final[['id', 'popularity', 'ww_rev']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_finale = movies_finale.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies_finale.to_csv(r'/Users/davestearns/Documents\\\\movies_finale.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reduce the size of the data and to reduce low value bias\n",
    "graph_df = movies_final[movies_final['ww_rev'] > 5000000]\n",
    "graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "populs = graph_df['popularity']\n",
    "revs = graph_df['ww_rev']\n",
    "graph2 = sns.scatterplot(x = revs, y = populs, hue=graph_df['popularity'], data = graph_df)\n",
    "graph2.set(xlabel='Revenue', ylabel='Popularity')\n",
    "graph2.set_xticklabels(['$0M', '$50M', '$100M', '$150M', '$200M', '$250M', '$300M', '$350M', '$400M'])\n",
    "plt.title('Comparing Revenue to Popularity')\n",
    "plt.xlim(50000000, 400000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlate the Revenue to Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_df['popularity'].corr(graph_df['ww_rev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a new dataframe to further reduce the influence on outliers\n",
    "# There weren't many movies above a popularity of 170, so to reduce outlier influence action was taken\n",
    "# Values below 5 were making the visualization hard to see - they were omitted\n",
    "# Vote Counts below 1000 were omitted to combat outlier influence\n",
    "movies_final2 = movies_final[(movies_final['popularity'] < 170) & (movies_final['popularity'] > 5) & (movies_final['vote_count'] > 1000)]\n",
    "pop = movies_final2['popularity']\n",
    "votes = movies_final2['vote_count']\n",
    "graph1 = sns.scatterplot(x = votes, y = pop, hue = movies_final2['vote_average'])\n",
    "graph1.set(xlabel='Vote Count', ylabel='Popularity')\n",
    "plt.title('Comparing Popularity to Vote Count')\n",
    "popvote_corr = movies_final2['vote_count'].corr(movies_final2['popularity'])\n",
    "popvote_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing what movie had the highest popularity, and looking at its revenue\n",
    "# EDA purposes\n",
    "movies_final2[movies_final2['popularity'] > 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusing previous dataframe for this visualization\n",
    "# Sorting it based on revenue, highest first\n",
    "# graph_df.sort_values('ww_rev', ascending=False, inplace=True)\n",
    "# Making a list of the first 10 names\n",
    "names = movies_final.index[0:10].to_list()\n",
    "top10_grossing_movies = sns.barplot(x=names, y=movies_final['ww_rev'][0:10])\n",
    "top10_grossing_movies.set_xticklabels(\n",
    "    top10_grossing_movies.get_xticklabels(),\n",
    "    rotation = 45, horizontalalignment = 'right')\n",
    "plt.ylim(20000000, 3000000000)\n",
    "y_labels = ['$0.5B', '$1.0B', '$1.5B', '$2.0B', '$2.5B', '$3.0B']\n",
    "top10_grossing_movies.set_yticklabels(y_labels)\n",
    "plt.title('Top 10 Grossing Movies (Worldwide Revenue)')\n",
    "top10_grossing_movies.set(xlabel='Top 10 Grossing Movies Worldwide', ylabel='Revenue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Profit Velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to convert datatypes to and from datetime objects\n",
    "# Needed for date analysis\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlist = movies_final['release_date'].to_list()\n",
    "movies_final['vlist'] = vlist\n",
    "movies_final.dropna(subset=['vlist'], inplace=True)\n",
    "vlist2 = movies_final['vlist'].to_list()\n",
    "# movies_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlist3 = []\n",
    "for v in vlist2:\n",
    "    vlist3.append(str(v))\n",
    "movies_final['vlist3'] = vlist3\n",
    "movies_final.dropna(inplace=True)\n",
    "vlist4 = movies_final['vlist3'].to_list()\n",
    "# vlist3\n",
    "vlistadd = []\n",
    "for v in vlist4:\n",
    "    vlistadd.append(datetime.strptime(v, \\\"%Y-%m-%d\\\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_final['release'] = vlistadd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_final.drop(columns = ['vlist', 'release_date'], axis = 1, inplace=True)\n",
    "movies_final.drop(columns='index', inplace=True)\n",
    "movies_final.drop(columns='vlist3', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = '2020-03-05'\n",
    "todate = datetime.strptime(today, \\\"%Y-%m-%d\\\")\n",
    "movies_final['release'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_final['time_elapsed'] = todate - movies_final['release']\n",
    "movies_final['time_elapsed'] = movies_final['time_elapsed'].apply(lambda x: int(x.days))\n",
    "movies_final['time_elapsed']\n",
    "movies_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_final['profit_velocity'] = movies_final['ww_rev']/movies_final['time_elapsed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing the dataset to include movies within the past 25 years\n",
    "velocity = movies_final[(movies_final['time_elapsed'] < 12000) & (movies_final['time_elapsed'] > 6000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = velocity.index[0:10].to_list()\n",
    "top10_grossing_movies = sns.barplot(x=names, y=velocity['profit_velocity'][0:10])\n",
    "top10_grossing_movies.set_xticklabels(\n",
    "    top10_grossing_movies.get_xticklabels(),\n",
    "    rotation = 45, horizontalalignment = 'right')\n",
    "plt.ylim(0, 350000)\n",
    "y_labels = ['$0', '$50K', '$100K', '$150K', '$200K', '$250K', '$300K']\n",
    "top10_grossing_movies.set_yticklabels(y_labels)\n",
    "plt.title('Top Movers of the Past Generation')\n",
    "top10_grossing_movies.set(xlabel='Top 10 Movers', ylabel='Profit Velocity ($/Day)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_new = velocity[velocity['ww_rev'] > 6000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_new['ww_rev'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_new.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
